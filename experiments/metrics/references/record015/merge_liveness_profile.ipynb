{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d98d0e",
   "metadata": {},
   "source": [
    "## Algorithm 1: Naive Paired Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "903d7ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_files(directory):\n",
    "    \"\"\"\n",
    "    Parses all 'ipra_analysis_*.txt' files in a directory. This is done by\n",
    "    reading all files to build a complete model of the program's call graph\n",
    "    and register usage.\n",
    "    \"\"\"\n",
    "    callee_save_costs = {}\n",
    "    callee_call_sites = defaultdict(list)\n",
    "    function_hotness = {}\n",
    "    tail_functions = {}\n",
    "\n",
    "    func_pattern = re.compile(r\"IPRA: Function: (.*?)\\[\")\n",
    "    usage_pattern = re.compile(r\"CSRegUsage: (.*?) IsFunctionEntryHot: (\\d+)\")\n",
    "    call_pattern = re.compile(r\"Calls: (.*?)\\[.*\\] IsTailCall: (\\d+).*? LivingCSRegs: (.*)\")\n",
    "    mbb_pattern = re.compile(r\"MBB: \\d+.*?MBBCount: (\\d+)\")\n",
    "\n",
    "    files_to_process = [os.path.join(directory, f) for f in os.listdir(directory) if f.startswith('ipra_analysis_') and f.endswith('.txt')]\n",
    "    print(f\"Found {len(files_to_process)} profile files to process.\")\n",
    "\n",
    "    for filepath in files_to_process:\n",
    "        with open(filepath, 'r', errors='ignore') as f:\n",
    "            current_function = None\n",
    "            current_mbb_count = 0\n",
    "            for line in f:\n",
    "                func_match = func_pattern.search(line)\n",
    "                if func_match:\n",
    "                    new_function = func_match.group(1).strip()\n",
    "                    if new_function != current_function:\n",
    "                        current_mbb_count = 0\n",
    "                    current_function = new_function\n",
    "\n",
    "                usage_match = usage_pattern.search(line)\n",
    "                if usage_match and current_function:\n",
    "                    regs_str = usage_match.group(1).strip()\n",
    "                    num_regs = len(regs_str.split()) if regs_str else 0\n",
    "                    callee_save_costs[current_function] = num_regs\n",
    "                    is_hot_str = usage_match.group(2).strip()\n",
    "                    function_hotness[current_function] = (int(is_hot_str) == 1)\n",
    "                \n",
    "                mbb_match = mbb_pattern.search(line)\n",
    "                if mbb_match:\n",
    "                    current_mbb_count = int(mbb_match.group(1))\n",
    "\n",
    "                call_match = call_pattern.search(line)\n",
    "                if call_match and current_function:\n",
    "                    callee_name = call_match.group(1).strip()\n",
    "                    is_tail_call = (int(call_match.group(2).strip()) == 1)\n",
    "                    tail_functions[current_function] = is_tail_call\n",
    "                    live_regs_str = call_match.group(3).strip()\n",
    "                    \n",
    "                    num_live_regs = len(live_regs_str.split()) if live_regs_str else 0\n",
    "                    callee_call_sites[callee_name].append({\n",
    "                        \"caller\": current_function,\n",
    "                        \"live_csrs\": num_live_regs,\n",
    "                        \"count\": current_mbb_count\n",
    "                    })\n",
    "\n",
    "    print(f\"Found callee-save costs for {len(callee_save_costs)} unique functions.\")\n",
    "    print(f\"Found call sites for {len(callee_call_sites)} unique callees.\")\n",
    "    return callee_save_costs, callee_call_sites, function_hotness, tail_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3632898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_benefits(callee_save_costs, callee_call_sites, function_hotness, tail_functions):\n",
    "    \"\"\"\n",
    "    Calculates the total adjusted benefit score for each function.\n",
    "    \"\"\"\n",
    "    benefit_scores = defaultdict(int)\n",
    "    print(f\"Calculating benefit scores...\")\n",
    "\n",
    "    # total_static_cost = 0\n",
    "\n",
    "    for callee, sites in callee_call_sites.items():\n",
    "        if not function_hotness.get(callee, False):\n",
    "            continue\n",
    "        callee_cost = callee_save_costs.get(callee, 0)\n",
    "        \n",
    "        total_dynamic_benefit = 0\n",
    "        sum_of_caller_costs = 0\n",
    "        \n",
    "        for site in sites:\n",
    "            caller_cost = site[\"live_csrs\"]\n",
    "            exec_count = site[\"count\"]\n",
    "            total_dynamic_benefit += (callee_cost - caller_cost) * exec_count\n",
    "            sum_of_caller_costs += caller_cost\n",
    "\n",
    "        # Calculate the total static cost (code size impact)\n",
    "        # It's the total number of new pushes/pops minus the ones removed.\n",
    "        # total_static_cost += 2 * (sum_of_caller_costs -  callee_cost)\n",
    "        \n",
    "        # Final adjusted score\n",
    "        # adjusted_score = total_dynamic_benefit\n",
    "        if total_dynamic_benefit > 0:\n",
    "            benefit_scores[callee] = total_dynamic_benefit\n",
    "        \n",
    "    return benefit_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1c3fd0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1833 profile files to process.\n",
      "Found callee-save costs for 93305 unique functions.\n",
      "Found call sites for 59511 unique callees.\n",
      "Calculating benefit scores...\n",
      "Found 1730 candidate functions meeting the threshold.\n"
     ]
    }
   ],
   "source": [
    "LIVENESS_DATA_DIR = './fdo_liveness_output'\n",
    "SIZE_PENALTY = 0.1\n",
    "PRESERVE_NONE_THRESHOLD = 0\n",
    "\n",
    "costs, sites, function_hotness, tail_functions = parse_files(LIVENESS_DATA_DIR)\n",
    "candidate_scores = calculate_benefits(costs, sites, function_hotness, tail_functions)\n",
    "\n",
    "output_data = dict(candidate_scores.items())\n",
    "print(f\"Found {len(candidate_scores)} candidate functions meeting the threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1d530e6f-7ec2-494d-890a-6bc561d5545c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 63 to discard\n",
      "defaultdict(<class 'int'>, {'HasAddressTaken': 63, 'AllUsesAreNotCall': 62, 'UsesAreIndirectCall': 1})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "    # Regex to capture the main components of a line\n",
    "    main_pattern = re.compile(r\"^IPRA: Function: (.+?)\\[(.*?)\\]\\s*(.*)$\")\n",
    "    # Regex to find all flag names within the flags part of the line\n",
    "    flag_pattern = re.compile(r\"(\\w+): \\d+\")\n",
    "    parsed_functions = {}\n",
    "    for filepath in files_to_process:\n",
    "        with open(filepath, 'r', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                main_match = main_pattern.match(line.strip())\n",
    "                \n",
    "                if main_match:\n",
    "                    func_name = main_match.group(1)\n",
    "                    cu_name = main_match.group(2)\n",
    "                    flags_string = main_match.group(3)\n",
    "                    present_flags = flag_pattern.findall(flags_string)\n",
    "                    \n",
    "                    parsed_functions[func_name] = present_flags\n",
    "    return parsed_functions\n",
    "\n",
    "\n",
    "parsed_functions = filter_dangerous_functions(LIVENESS_PRERA_DATA_DIR)\n",
    "discarded_functions = set()\n",
    "discarded_counts = defaultdict(int)\n",
    "discarded_flags = {'HasAddressTaken', 'MustTailCall', 'IsInterposable', 'UsesAreIndirectCall', 'AllUsesAreNotCall'}\n",
    "for func in output_data:\n",
    "    if func in discarded_functions or func not in parsed_functions:\n",
    "        continue\n",
    "    flags = set(parsed_functions[func])\n",
    "    intersection = flags.intersection(discarded_flags)\n",
    "    if not intersection:\n",
    "        continue\n",
    "    for flag in intersection:\n",
    "        discarded_counts[flag] += 1\n",
    "    discarded_functions.add(func)\n",
    "print(f\"found {len(discarded_functions)} to discard\")\n",
    "print(discarded_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "019f1931-0409-41d5-92d5-bda72609e3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 63 functions. Now total 1667 functions left.\n"
     ]
    }
   ],
   "source": [
    "before_len = len(output_data)\n",
    "for func in discarded_functions:\n",
    "    output_data.pop(func)\n",
    "after_len = len(output_data)\n",
    "print(f\"Filtered out {before_len - after_len} functions. Now total {len(output_data)} functions left.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a6955051-e0b1-4c21-9be9-6709a1f81548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final filtering, 1667 functions.\n",
      "\n",
      "✅ Successfully merged profile data into './fdo_liveness_output/liveness_profdata.json'\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_FILE = f'{LIVENESS_DATA_DIR}/liveness_profdata.json'\n",
    "filtered_output_data = {func: score for func, score in output_data.items()}\n",
    "print(f\"Final filtering, {len(filtered_output_data)} functions.\")\n",
    "output_dict = {\"functions\": filtered_output_data}\n",
    "with open(OUTPUT_FILE, 'w') as f:\n",
    "    json.dump(output_dict, f, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Successfully merged profile data into '{OUTPUT_FILE}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f884aa6",
   "metadata": {},
   "source": [
    "## Algorithm 2: Propagating Costs via Bottom-Up Call Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff3068ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_files(directory):\n",
    "    \"\"\"\n",
    "    Parses all 'ipra_analysis_*.txt' files in a directory to build a model\n",
    "    of the program's call graph and register usage.\n",
    "    \"\"\"\n",
    "    function_regs = {}\n",
    "    function_hotness = {} # Store hotness for each function\n",
    "    callee_call_sites = defaultdict(list)\n",
    "    # The call graph is represented as Caller -> set(Callees)\n",
    "    successors = defaultdict(set)\n",
    "    predecessors = defaultdict(set)\n",
    "\n",
    "    func_pattern = re.compile(r\"IPRA: Function: (.*?)\\[\")\n",
    "    usage_pattern = re.compile(r\"CSRegUsage: (.*?) IsFunctionEntryHot: (\\d+) (?:EntryCount: (\\d+))?\")\n",
    "    call_pattern = re.compile(r\"Calls: (.*?)\\[.*\\] IsTailCall: (\\d+).*? LivingCSRegs: (.*)\")\n",
    "    mbb_pattern = re.compile(r\"MBB: \\d+.*?MBBCount: (\\d+)\")\n",
    "\n",
    "    files_to_process = [os.path.join(directory, f) for f in os.listdir(directory) if f.startswith('ipra_analysis_') and f.endswith('.txt')]\n",
    "    print(f\"Found {len(files_to_process)} profile files to process.\")\n",
    "\n",
    "    all_functions = set()\n",
    "\n",
    "    for filepath in files_to_process:\n",
    "        with open(filepath, 'r', errors='ignore') as f:\n",
    "            current_function = None\n",
    "            current_mbb_count = 0\n",
    "            for line in f:\n",
    "                func_match = func_pattern.search(line)\n",
    "                if func_match:\n",
    "                    new_function = func_match.group(1).strip()\n",
    "                    if new_function != current_function:\n",
    "                        current_mbb_count = 0\n",
    "                    current_function = new_function\n",
    "                    all_functions.add(current_function)\n",
    "                    \n",
    "\n",
    "                usage_match = usage_pattern.search(line)\n",
    "                if usage_match and current_function:\n",
    "                    regs_str = usage_match.group(1).strip()\n",
    "                    regs_list = regs_str.split() if regs_str else []\n",
    "                    function_regs[current_function] = set(regs_list)\n",
    "                    is_hot = (int(usage_match.group(2).strip()) == 1)\n",
    "                    if is_hot:\n",
    "                        function_entry_count = int(usage_match.group(3).strip())\n",
    "                    else:\n",
    "                        function_entry_count = 1\n",
    "                    function_hotness[current_function] = function_entry_count\n",
    "                    \n",
    "                \n",
    "                mbb_match = mbb_pattern.search(line)\n",
    "                if mbb_match:\n",
    "                    current_mbb_count = int(mbb_match.group(1))\n",
    "\n",
    "                call_match = call_pattern.search(line)\n",
    "                if call_match and current_function:\n",
    "                    callee_name = call_match.group(1).strip()\n",
    "                    is_tail_call_str = call_match.group(2).strip()\n",
    "                    \n",
    "                    all_functions.add(callee_name)\n",
    "                    live_regs_str = call_match.group(3).strip()\n",
    "                    live_regs_list = live_regs_str.split() if live_regs_str else []\n",
    "                    \n",
    "                    callee_call_sites[callee_name].append({\n",
    "                        \"caller\": current_function,\n",
    "                        \"live_csrs\": set(live_regs_list),\n",
    "                        \"count\": current_mbb_count,\n",
    "                        \"is_tail_call\": (int(is_tail_call_str) == 1)\n",
    "                    })\n",
    "                    successors[current_function].add(callee_name)\n",
    "                    predecessors[callee_name].add(current_function)\n",
    "\n",
    "    print(f\"Found {len(all_functions)} unique functions in the call graph.\")\n",
    "    return function_regs, callee_call_sites, successors, predecessors, all_functions, function_hotness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21a550c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "\n",
    "def tarjan_scc(nodes, successors):\n",
    "    \"\"\"\n",
    "    Tarjan's algorithm.\n",
    "    Args:\n",
    "        nodes: iterable of node ids (e.g., function names)\n",
    "        successors: dict[node] -> set[node] (caller -> callees)\n",
    "    Returns:\n",
    "        sccs: list[list[node]] where each inner list is one SCC (arbitrary order)\n",
    "        comp_id: dict[node] -> int  component index for each node\n",
    "    \"\"\"\n",
    "    index = 0\n",
    "    idx = {}\n",
    "    low = {}\n",
    "    onstack = set()\n",
    "    stack = []\n",
    "    sccs = []\n",
    "\n",
    "    def strongconnect(v):\n",
    "        nonlocal index\n",
    "        idx[v] = index\n",
    "        low[v] = index\n",
    "        index += 1\n",
    "        stack.append(v)\n",
    "        onstack.add(v)\n",
    "\n",
    "        for w in successors.get(v, ()):\n",
    "            if w not in idx:\n",
    "                strongconnect(w)\n",
    "                low[v] = min(low[v], low[w])\n",
    "            elif w in onstack:\n",
    "                low[v] = min(low[v], idx[w])\n",
    "\n",
    "        # root of an SCC\n",
    "        if low[v] == idx[v]:\n",
    "            component = []\n",
    "            while True:\n",
    "                w = stack.pop()\n",
    "                onstack.remove(w)\n",
    "                component.append(w)\n",
    "                if w == v:\n",
    "                    break\n",
    "            sccs.append(component)\n",
    "\n",
    "    for v in nodes:\n",
    "        if v not in idx:\n",
    "            strongconnect(v)\n",
    "\n",
    "    # component id map\n",
    "    comp_id = {}\n",
    "    for cid, comp in enumerate(sccs):\n",
    "        for v in comp:\n",
    "            comp_id[v] = cid\n",
    "    return sccs, comp_id\n",
    "\n",
    "\n",
    "def build_condensed_dag(nodes, successors, predecessors):\n",
    "    \"\"\"\n",
    "    Collapse nodes into SCC super-nodes and build the condensed DAG.\n",
    "    Returns:\n",
    "        sccs: list[list[node]]  original nodes per component id\n",
    "        comp_id: dict[node]->int\n",
    "        dag_succ: dict[cid] -> set[cid]\n",
    "        dag_pred: dict[cid] -> set[cid]\n",
    "    \"\"\"\n",
    "    sccs, comp_id = tarjan_scc(nodes, successors)\n",
    "\n",
    "    dag_succ = defaultdict(set)\n",
    "    dag_pred = defaultdict(set)\n",
    "\n",
    "    for u in nodes:\n",
    "        cu = comp_id[u]\n",
    "        for v in successors.get(u, ()):\n",
    "            cv = comp_id[v]\n",
    "            if cu != cv:\n",
    "                dag_succ[cu].add(cv)\n",
    "                dag_pred[cv].add(cu)\n",
    "\n",
    "    # ensure every component appears in maps\n",
    "    C = len(sccs)\n",
    "    for c in range(C):\n",
    "        _ = dag_succ[c]\n",
    "        _ = dag_pred[c]\n",
    "\n",
    "    return sccs, comp_id, dag_succ, dag_pred\n",
    "\n",
    "\n",
    "def scc_order_bottom_up(dag_succ, dag_pred):\n",
    "    \"\"\"\n",
    "    Bottom-up order on the SCC DAG: sinks first (no outgoing edges).\n",
    "    Uses a Kahn-style peel from sinks upward.\n",
    "    Returns:\n",
    "        order: list[int] of component ids in bottom-up order.\n",
    "    \"\"\"\n",
    "    all_cids = set(dag_succ) | set(dag_pred)\n",
    "    outdeg = {c: len(dag_succ.get(c, ())) for c in all_cids}\n",
    "    q = deque([c for c in all_cids if outdeg[c] == 0])\n",
    "    order = []\n",
    "\n",
    "    while q:\n",
    "        u = q.popleft()\n",
    "        order.append(u)\n",
    "        for p in dag_pred.get(u, ()):\n",
    "            outdeg[p] -= 1\n",
    "            if outdeg[p] == 0:\n",
    "                q.append(p)\n",
    "\n",
    "    # If cycles existed here, something's wrong—condensation must be a DAG.\n",
    "    # But to be safe, append any unprocessed nodes (should be none).\n",
    "    if len(order) != len(all_cids):\n",
    "        remaining = [c for c in all_cids if c not in set(order)]\n",
    "        order.extend(remaining)\n",
    "    return order\n",
    "\n",
    "\n",
    "def function_order_bottom_up(nodes, successors, predecessors):\n",
    "    \"\"\"\n",
    "    Convenience wrapper: returns\n",
    "      - sccs (list of lists of original nodes),\n",
    "      - bottom-up SCC id order,\n",
    "      - a flattened bottom-up function order (each SCC kept as a group).\n",
    "    \"\"\"\n",
    "    sccs, comp_id, dag_succ, dag_pred = build_condensed_dag(nodes, successors, predecessors)\n",
    "    scc_bottom_up = scc_order_bottom_up(dag_succ, dag_pred)\n",
    "\n",
    "    # Flatten functions in bottom-up SCC order.\n",
    "    # For multi-node SCCs (recursion), return them as a list to process as a unit.\n",
    "    grouped = [list(sccs[cid]) for cid in scc_bottom_up]\n",
    "    flat = [f for group in grouped for f in group]  # if you really need a flat list\n",
    "\n",
    "    return {\n",
    "        \"sccs\": sccs,\n",
    "        \"scc_bottom_up_order\": scc_bottom_up,\n",
    "        \"grouped_functions_bottom_up\": grouped,\n",
    "        \"flat_functions_bottom_up\": flat,\n",
    "        \"comp_id\": comp_id,\n",
    "        \"condensed_successors\": dag_succ,\n",
    "        \"condensed_predecessors\": dag_pred,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605b9cde-2d3b-485b-96e6-52f9252cc098",
   "metadata": {},
   "source": [
    "### Algo: Naively select functions that are hot and used up 6 registers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4550c84-7bba-4870-9cb0-84bd24004d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_benefits_bottom_up(callee_save_regs, callee_call_sites, successors, predecessors, all_functions, function_hotness, size_penalty, threshold):\n",
    "#     res = function_order_bottom_up(all_functions, successors, predecessors)\n",
    "#     sorted_nodes = res[\"flat_functions_bottom_up\"]\n",
    "#     print(f\"Topologically sorted {len(sorted_nodes)} functions for bottom-up processing.\")\n",
    "\n",
    "#     final_candidates = set()\n",
    "\n",
    "#     for callee in sorted_nodes:\n",
    "#         if function_hotness.get(callee, 1) > 1 and len(callee_save_regs.get(callee, [])) >= 6:\n",
    "#             final_candidates.add(callee)\n",
    "#     return {func: 1000000 for func in final_candidates}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae62f975-a211-4c1b-9679-0828750c6757",
   "metadata": {},
   "source": [
    "### Algo: Bottom Up CFG Propagate Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26565542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_benefits_bottom_up(function_regs, callee_call_sites, successors, predecessors, all_functions, function_hotness, size_penalty, threshold):\n",
    "    \"\"\"\n",
    "    Calculates benefit scores using a bottom-up traversal of the call graph\n",
    "    to model the cascading effects of the preserve_none optimization.\n",
    "    \"\"\"\n",
    "    res = function_order_bottom_up(all_functions, successors, predecessors)\n",
    "    sorted_nodes = res[\"flat_functions_bottom_up\"]\n",
    "    print(f\"Topologically sorted {len(sorted_nodes)} functions for bottom-up processing.\")\n",
    "\n",
    "    final_candidates = set()\n",
    "    final_scores = {}\n",
    "\n",
    "    # --- MODIFICATION START ---\n",
    "    # 1. Get the list of SCCs from the graph analysis result.\n",
    "    sccs = res[\"sccs\"]\n",
    "\n",
    "    # 2. Create a set of all functions that belong to a multi-node SCC (a cycle).\n",
    "    functions_in_cycles = set()\n",
    "    for scc in sccs:\n",
    "        if len(scc) > 1:\n",
    "            for func_in_cycle in scc:\n",
    "                functions_in_cycles.add(func_in_cycle)\n",
    "    \n",
    "    if functions_in_cycles:\n",
    "        print(f\"Identified {len(functions_in_cycles)} functions in recursive cycles. They will be skipped.\")\n",
    "    # --- MODIFICATION END ---\n",
    "    for callee in sorted_nodes:\n",
    "        # --- MODIFICATION START ---\n",
    "        # 3. Skip any function that is part of a recursive cycle.\n",
    "        if callee in functions_in_cycles:\n",
    "            final_scores[callee] = float('-inf') # Assign a score indicating it was skipped\n",
    "            continue\n",
    "        # --- MODIFICATION END ---\n",
    "        call_sites_for_callee = callee_call_sites.get(callee, [])\n",
    "\n",
    "        # # Skip any non-hot function (your original logic):\n",
    "        # if not function_hotness.get(callee, False):\n",
    "        #     final_scores[callee] = float('-inf')\n",
    "        #     continue\n",
    "\n",
    "        # Ideally should not skip tail calls\n",
    "        # is_tail_call = any([site[\"is_tail_call\"] for site in call_sites_for_callee])\n",
    "        # if is_tail_call:\n",
    "        #     continue\n",
    "        \n",
    "        # # If any of the caller is hot, also skip --- Experimental\n",
    "        # has_caller_hot = False\n",
    "        # callers = {site[\"caller\"] for site in call_sites_for_callee}\n",
    "        # for caller in callers:\n",
    "        #     if function_hotness.get(caller, False):\n",
    "        #         has_caller_hot = True\n",
    "        #         break\n",
    "        # if has_caller_hot:\n",
    "        #     continue\n",
    "        callee_regs_set = function_regs.get(callee, [])\n",
    "        callee_cost = len(callee_regs_set)\n",
    "        total_dynamic_benefit = 0\n",
    "        sum_of_caller_static_costs = 0\n",
    "        \n",
    "        for site in call_sites_for_callee:\n",
    "            caller_name = site[\"caller\"]\n",
    "            live_regs_set = site[\"live_csrs\"]\n",
    "            caller_live_regs_set = live_regs_set.intersection(callee_regs_set)\n",
    "            caller_live_regs_cost = len(caller_live_regs_set)\n",
    "            caller_function_regs_cost = 6 - len(function_regs.get(caller_name, []))\n",
    "            caller_entry_count = function_hotness.get(caller_name, 1)\n",
    "            mbb_count = site[\"count\"]\n",
    "            total_dynamic_benefit += 2 * (callee_cost - caller_live_regs_cost) * mbb_count - caller_function_regs_cost * caller_entry_count\n",
    "            sum_of_caller_static_costs += caller_live_regs_cost + caller_function_regs_cost\n",
    "\n",
    "        total_static_cost = max(2 * (sum_of_caller_static_costs - callee_cost), 0)\n",
    "        # if total_static_cost > 0:\n",
    "        #     continue\n",
    "        adjusted_score = total_dynamic_benefit - (size_penalty * total_static_cost)\n",
    "        final_scores[callee] = adjusted_score\n",
    "\n",
    "        if adjusted_score > 0:\n",
    "            final_candidates.add(callee)\n",
    "            \n",
    "            original_callee_cost = len(function_regs.get(callee, []))\n",
    "            for caller in predecessors[callee]:\n",
    "                function_regs[caller] = {\"$rbx\", \"rbp\", \"$r12\", \"$r13\", \"$r14\", \"$r15\"}\n",
    "\n",
    "    return {func: score for func, score in final_scores.items() if func in final_candidates and score > threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df575f44-dc6a-46a8-83ba-854a9dec871a",
   "metadata": {},
   "source": [
    "## Algo: Also bottom-up but treat SCC as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8a961a1-d1db-4f23-b3d5-bf1dff9ed9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# def calculate_benefits_bottom_up(\n",
    "#     callee_save_costs, callee_call_sites, successors, predecessors, all_functions,\n",
    "#     function_hotness, size_penalty, threshold\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Calculates benefit scores using a bottom-up traversal of the call graph's SCCs.\n",
    "#     It treats recursive cycles (multi-node SCCs) as single \"super-nodes\",\n",
    "#     making an all-or-nothing optimization decision for the entire cycle.\n",
    "#     \"\"\"\n",
    "#     # Perform graph analysis to get SCCs and a bottom-up processing order.\n",
    "#     graph_info = function_order_bottom_up(all_functions, successors, predecessors)\n",
    "#     sccs = graph_info[\"sccs\"]\n",
    "#     scc_bottom_up_order = graph_info[\"scc_bottom_up_order\"]\n",
    "#     comp_id_map = graph_info[\"comp_id\"]\n",
    "#     print(f\"Processing {len(sccs)} SCCs in bottom-up order.\")\n",
    "\n",
    "#     # final_candidates will store the functions chosen for preserve_none.\n",
    "#     final_candidates = set()\n",
    "#     # effective_cs_usage models the \"register pressure\" propagated up the call graph.\n",
    "#     effective_cs_usage = defaultdict(int, callee_save_costs)\n",
    "#     # final_scores stores the calculated score for every function/SCC.\n",
    "#     final_scores = {}\n",
    "\n",
    "#     # IMPROVEMENT: Iterate over SCCs, not flattened functions.\n",
    "#     # This allows us to handle single functions and recursive cycles differently.\n",
    "#     for scc_id in scc_bottom_up_order:\n",
    "#         scc = sccs[scc_id]\n",
    "\n",
    "#         # --- Pre-computation for the current SCC (single or super-node) ---\n",
    "        \n",
    "#         # Heuristic: If any function in an SCC is not hot or has a tail call,\n",
    "#         # it's often safer and simpler to disqualify the entire SCC.\n",
    "#         # This prevents applying preserve_none to functions that are part of a\n",
    "#         # sensitive or cold cycle.\n",
    "#         # scc_is_hot = all(function_hotness.get(func, False) for func in scc)\n",
    "\n",
    "#         # Experimental: We probably want to process tail call\n",
    "#         # has_tail_call = any(\n",
    "#         #     site.get(\"is_tail_call\", False)\n",
    "#         #     for func in scc\n",
    "#         #     for site in callee_call_sites.get(func, [])\n",
    "#         # )\n",
    "\n",
    "#         # if not scc_is_hot: #or has_tail_call\n",
    "#         #     for func in scc:\n",
    "#         #         final_scores[func] = float('-inf') # Mark as disqualified\n",
    "#         #     continue\n",
    "\n",
    "#         # --- Cost/Benefit Analysis ---\n",
    "        \n",
    "#         # These will hold the aggregated scores for the entire SCC.\n",
    "#         # For a single-function SCC, they just represent that one function.\n",
    "#         total_dynamic_benefit = 0\n",
    "#         sum_of_caller_static_costs = 0\n",
    "        \n",
    "#         # The total intrinsic cost of the SCC is the sum of saves of its members.\n",
    "#         # This is what gets pushed up to callers if the SCC is chosen.\n",
    "#         scc_original_cost = sum(callee_save_costs.get(func, 0) for func in scc)\n",
    "        \n",
    "#         # The current cost, including costs propagated from callees.\n",
    "#         scc_effective_cost = sum(effective_cs_usage.get(func, 0) for func in scc)\n",
    "\n",
    "#         # Calculate benefit by looking at all call sites *calling into* the SCC.\n",
    "#         for func_in_scc in scc:\n",
    "#             for site in callee_call_sites.get(func_in_scc, []):\n",
    "#                 caller_name = site[\"caller\"]\n",
    "                \n",
    "#                 # Ignore calls from within the same SCC (internal recursion).\n",
    "#                 # We only care about the SCC's external interface.\n",
    "#                 if comp_id_map.get(caller_name) == scc_id:\n",
    "#                     continue\n",
    "\n",
    "#                 # The cost for an external caller to save registers.\n",
    "#                 # Heuristic: 6 (total CSRs) - registers already live in caller.\n",
    "#                 caller_register_headroom = 6 - site[\"live_csrs\"]\n",
    "#                 exec_count = site[\"count\"]\n",
    "                \n",
    "#                 # The benefit is based on the *entire* SCC's effective cost,\n",
    "#                 # as the caller must now guard against all registers used in the cycle.\n",
    "#                 total_dynamic_benefit += (scc_effective_cost - caller_register_headroom) * exec_count\n",
    "#                 sum_of_caller_static_costs += caller_register_headroom\n",
    "\n",
    "#         # Static cost estimates the code size impact.\n",
    "#         # If total_static_cost is negative, we should only consider dynamic cost because less static_cost is definitely a win.\n",
    "#         # So we keep total_static_cost non_negative\n",
    "#         total_static_cost = max((2 * sum_of_caller_static_costs) - (2 * scc_effective_cost), 0)\n",
    "#         #Experimental: if static cost increases, we will skip\n",
    "#         if (2 * sum_of_caller_static_costs) - (2 * scc_effective_cost) > 0:\n",
    "#             continue\n",
    "\n",
    "#         adjusted_score = total_dynamic_benefit - (size_penalty * total_static_cost)\n",
    "        \n",
    "#         # Store score for all functions in the SCC for logging/debugging.\n",
    "#         for func in scc:\n",
    "#             final_scores[func] = adjusted_score\n",
    "\n",
    "#         # --- Decision and Cost Propagation ---\n",
    "        \n",
    "#         # IMPROVEMENT: Make one \"all-or-nothing\" decision for the SCC.\n",
    "#         if adjusted_score > threshold:\n",
    "#             # If the score is positive, mark ALL functions in the SCC as candidates.\n",
    "#             for func in scc:\n",
    "#                 final_candidates.add(func)\n",
    "            \n",
    "#             # Propagate the combined original cost of the entire SCC to all\n",
    "#             # external callers.\n",
    "#             callers_of_scc = {\n",
    "#                 site[\"caller\"]\n",
    "#                 for func in scc\n",
    "#                 for site in callee_call_sites.get(func, [])\n",
    "#                 if comp_id_map.get(site[\"caller\"]) != scc_id\n",
    "#             }\n",
    "            \n",
    "#             for caller in callers_of_scc:\n",
    "#                 effective_cs_usage[caller] += scc_original_cost\n",
    "\n",
    "#     # Filter the final results based on the threshold and candidate set.\n",
    "#     return {\n",
    "#         func: score\n",
    "#         for func, score in final_scores.items()\n",
    "#         if func in final_candidates and score > threshold\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6416cb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1686 profile files to process.\n",
      "Found 82923 unique functions in the call graph.\n",
      "Topologically sorted 82923 functions for bottom-up processing.\n",
      "Identified 1189 functions in recursive cycles. They will be skipped.\n",
      "Found 1213 candidate functions meeting the threshold.\n"
     ]
    }
   ],
   "source": [
    "LIVENESS_DATA_DIR = './thinly_linked_fdo_liveness_output'\n",
    "SIZE_PENALTY = 100000\n",
    "PRESERVE_NONE_THRESHOLD = 0\n",
    "\n",
    "costs, sites, successors, predecessors, all_nodes, function_hotness = parse_files(LIVENESS_DATA_DIR)\n",
    "\n",
    "candidate_scores = calculate_benefits_bottom_up(costs, sites, successors, predecessors, all_nodes, function_hotness, SIZE_PENALTY, PRESERVE_NONE_THRESHOLD)\n",
    "\n",
    "output_data = dict(candidate_scores.items())\n",
    "\n",
    "print(f\"Found {len(candidate_scores)} candidate functions meeting the threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b845e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 50 to discard\n",
      "defaultdict(<class 'int'>, {'AllUsesAreNotCall': 50, 'HasAddressTaken': 50})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "LIVENESS_PRERA_DATA_DIR = './thinly_linked_fdo_liveness_output'\n",
    "def filter_dangerous_functions(directory):\n",
    "    files_to_process = [os.path.join(directory, f) for f in os.listdir(directory) if f.startswith('ipra_prera_analysis_') and f.endswith('.txt')]\n",
    "    # Regex to capture the main components of a line\n",
    "    main_pattern = re.compile(r\"^IPRA: Function: (.+?)\\[(.*?)\\]\\s*(.*)$\")\n",
    "    # Regex to find all flag names within the flags part of the line\n",
    "    flag_pattern = re.compile(r\"(\\w+): \\d+\")\n",
    "    parsed_functions = {}\n",
    "    for filepath in files_to_process:\n",
    "        with open(filepath, 'r', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                main_match = main_pattern.match(line.strip())\n",
    "                \n",
    "                if main_match:\n",
    "                    func_name = main_match.group(1)\n",
    "                    cu_name = main_match.group(2)\n",
    "                    flags_string = main_match.group(3)\n",
    "                    present_flags = flag_pattern.findall(flags_string)\n",
    "                    \n",
    "                    parsed_functions[func_name] = present_flags\n",
    "    return parsed_functions\n",
    "\n",
    "\n",
    "parsed_functions = filter_dangerous_functions(LIVENESS_PRERA_DATA_DIR)\n",
    "discarded_functions = set()\n",
    "discarded_counts = defaultdict(int)\n",
    "discarded_flags = {'HasAddressTaken', 'MustTailCall', 'IsInterposable', 'UsesAreIndirectCall', 'AllUsesAreNotCall'}\n",
    "for func in output_data:\n",
    "    if func in discarded_functions or func not in parsed_functions:\n",
    "        continue\n",
    "    flags = set(parsed_functions[func])\n",
    "    intersection = flags.intersection(discarded_flags)\n",
    "    if not intersection:\n",
    "        continue\n",
    "    for flag in intersection:\n",
    "        discarded_counts[flag] += 1\n",
    "    discarded_functions.add(func)\n",
    "print(f\"found {len(discarded_functions)} to discard\")\n",
    "print(discarded_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e98f8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 50 functions. Now total 1163 functions left.\n"
     ]
    }
   ],
   "source": [
    "before_len = len(output_data)\n",
    "for func in discarded_functions:\n",
    "    output_data.pop(func)\n",
    "after_len = len(output_data)\n",
    "print(f\"Filtered out {before_len - after_len} functions. Now total {len(output_data)} functions left.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47702486-4d58-440b-bc40-9ffc1f102bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 115 low-performing functions. Now total 1048 functions left.\n"
     ]
    }
   ],
   "source": [
    "# Filter out low score function\n",
    "SCORE_THRESHOLD = 100000\n",
    "before_len = len(output_data)\n",
    "filtered_data = {func: score for func, score in output_data.items() if score > SCORE_THRESHOLD}\n",
    "after_len = len(filtered_data)\n",
    "print(f\"Filtered out {before_len - after_len} low-performing functions. Now total {len(filtered_data)} functions left.\")\n",
    "# filtered_data = output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d5bb9af-71a4-47d2-999c-e82dbcb5ee2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 0 bad functions. Now total 1935 functions left.\n"
     ]
    }
   ],
   "source": [
    "# Filter out known bad functions\n",
    "BAD_FUNCTION_PATH=f'{LIVENESS_DATA_DIR}/bad_functions.txt'\n",
    "before_len = len(filtered_data)\n",
    "with open(BAD_FUNCTION_PATH, 'r') as file:\n",
    "    for line in file:\n",
    "        bad_func = line.strip()\n",
    "        if bad_func in filtered_data:\n",
    "            filtered_data.pop(bad_func)\n",
    "after_len = len(filtered_data)\n",
    "print(f\"Filtered out {before_len - after_len} bad functions. Now total {len(filtered_data)} functions left.\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ff5e36",
   "metadata": {},
   "source": [
    "## Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b79cd502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Successfully merged profile data into './thinly_linked_fdo_liveness_output/liveness_profdata.json'\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_FILE = f'{LIVENESS_DATA_DIR}/liveness_profdata.json'\n",
    "output_dict = {\"functions\": filtered_data}\n",
    "with open(OUTPUT_FILE, 'w') as f:\n",
    "    json.dump(output_dict, f, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Successfully merged profile data into '{OUTPUT_FILE}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13a0981f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created './thinly_linked_fdo_liveness_output/pn.syms' with 1935 function symbols.\n"
     ]
    }
   ],
   "source": [
    "# For pn.syms\n",
    "function_names = list(filtered_data.keys())\n",
    "PN_SYMS_OUTPUT_PATH = f'{LIVENESS_DATA_DIR}/pn.syms'\n",
    "with open(PN_SYMS_OUTPUT_PATH, 'w') as f:\n",
    "    for name in function_names:\n",
    "        f.write(name + '\\n')\n",
    "print(f\"Successfully created '{PN_SYMS_OUTPUT_PATH}' with {len(function_names)} function symbols.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8922c9ca-0429-4b43-aa3d-2967e489f287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
