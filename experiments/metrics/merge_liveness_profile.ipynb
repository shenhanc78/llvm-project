{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d98d0e",
   "metadata": {},
   "source": [
    "## Algorithm 1: Naive Paired Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "903d7ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_files(directory):\n",
    "    \"\"\"\n",
    "    Parses all 'ipra_analysis_*.txt' files in a directory. This is done by\n",
    "    reading all files to build a complete model of the program's call graph\n",
    "    and register usage.\n",
    "    \"\"\"\n",
    "    callee_save_costs = {}\n",
    "    callee_call_sites = defaultdict(list)\n",
    "    function_hotness = {}\n",
    "    tail_functions = {}\n",
    "\n",
    "    func_pattern = re.compile(r\"IPRA: Function: (.*?)\\[\")\n",
    "    usage_pattern = re.compile(r\"CSRegUsage: (.*?) IsFunctionEntryHot: (\\d+)\")\n",
    "    call_pattern = re.compile(r\"Calls: (.*?)\\[.*\\] IsTailCall: (\\d+).*? LivingCSRegs: (.*)\")\n",
    "    mbb_pattern = re.compile(r\"MBB: \\d+.*?MBBCount: (\\d+)\")\n",
    "\n",
    "    files_to_process = [os.path.join(directory, f) for f in os.listdir(directory) if f.startswith('ipra_analysis_') and f.endswith('.txt')]\n",
    "    print(f\"Found {len(files_to_process)} profile files to process.\")\n",
    "\n",
    "    for filepath in files_to_process:\n",
    "        with open(filepath, 'r', errors='ignore') as f:\n",
    "            current_function = None\n",
    "            current_mbb_count = 0\n",
    "            for line in f:\n",
    "                func_match = func_pattern.search(line)\n",
    "                if func_match:\n",
    "                    new_function = func_match.group(1).strip()\n",
    "                    if new_function != current_function:\n",
    "                        current_mbb_count = 0\n",
    "                    current_function = new_function\n",
    "\n",
    "                usage_match = usage_pattern.search(line)\n",
    "                if usage_match and current_function:\n",
    "                    regs_str = usage_match.group(1).strip()\n",
    "                    num_regs = len(regs_str.split()) if regs_str else 0\n",
    "                    callee_save_costs[current_function] = num_regs\n",
    "                    is_hot_str = usage_match.group(2).strip()\n",
    "                    function_hotness[current_function] = (int(is_hot_str) == 1)\n",
    "                \n",
    "                mbb_match = mbb_pattern.search(line)\n",
    "                if mbb_match:\n",
    "                    current_mbb_count = int(mbb_match.group(1))\n",
    "\n",
    "                call_match = call_pattern.search(line)\n",
    "                if call_match and current_function:\n",
    "                    callee_name = call_match.group(1).strip()\n",
    "                    is_tail_call = (int(call_match.group(2).strip()) == 1)\n",
    "                    tail_functions[current_function] = is_tail_call\n",
    "                    live_regs_str = call_match.group(3).strip()\n",
    "                    \n",
    "                    num_live_regs = len(live_regs_str.split()) if live_regs_str else 0\n",
    "                    callee_call_sites[callee_name].append({\n",
    "                        \"caller\": current_function,\n",
    "                        \"live_csrs\": num_live_regs,\n",
    "                        \"count\": current_mbb_count\n",
    "                    })\n",
    "\n",
    "    print(f\"Found callee-save costs for {len(callee_save_costs)} unique functions.\")\n",
    "    print(f\"Found call sites for {len(callee_call_sites)} unique callees.\")\n",
    "    return callee_save_costs, callee_call_sites, function_hotness, tail_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3632898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_benefits(callee_save_costs, callee_call_sites, function_hotness, tail_functions):\n",
    "    \"\"\"\n",
    "    Calculates the total adjusted benefit score for each function.\n",
    "    \"\"\"\n",
    "    benefit_scores = defaultdict(int)\n",
    "    print(f\"Calculating benefit scores...\")\n",
    "\n",
    "    # total_static_cost = 0\n",
    "\n",
    "    for callee, sites in callee_call_sites.items():\n",
    "        if not function_hotness.get(callee, False):\n",
    "            continue\n",
    "        callee_cost = callee_save_costs.get(callee, 0)\n",
    "        \n",
    "        total_dynamic_benefit = 0\n",
    "        sum_of_caller_costs = 0\n",
    "        \n",
    "        for site in sites:\n",
    "            caller_cost = site[\"live_csrs\"]\n",
    "            exec_count = site[\"count\"]\n",
    "            total_dynamic_benefit += (callee_cost - caller_cost) * exec_count\n",
    "            sum_of_caller_costs += caller_cost\n",
    "\n",
    "        # Calculate the total static cost (code size impact)\n",
    "        # It's the total number of new pushes/pops minus the ones removed.\n",
    "        # total_static_cost += 2 * (sum_of_caller_costs -  callee_cost)\n",
    "        \n",
    "        # Final adjusted score\n",
    "        # adjusted_score = total_dynamic_benefit\n",
    "        if total_dynamic_benefit > 0:\n",
    "            benefit_scores[callee] = total_dynamic_benefit\n",
    "        \n",
    "    return benefit_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1c3fd0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1833 profile files to process.\n",
      "Found callee-save costs for 93305 unique functions.\n",
      "Found call sites for 59511 unique callees.\n",
      "Calculating benefit scores...\n",
      "Found 1730 candidate functions meeting the threshold.\n"
     ]
    }
   ],
   "source": [
    "LIVENESS_DATA_DIR = './fdo_liveness_output'\n",
    "SIZE_PENALTY = 0.1\n",
    "PRESERVE_NONE_THRESHOLD = 0\n",
    "\n",
    "costs, sites, function_hotness, tail_functions = parse_files(LIVENESS_DATA_DIR)\n",
    "candidate_scores = calculate_benefits(costs, sites, function_hotness, tail_functions)\n",
    "\n",
    "output_data = dict(candidate_scores.items())\n",
    "print(f\"Found {len(candidate_scores)} candidate functions meeting the threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1d530e6f-7ec2-494d-890a-6bc561d5545c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 63 to discard\n",
      "defaultdict(<class 'int'>, {'HasAddressTaken': 63, 'AllUsesAreNotCall': 62, 'UsesAreIndirectCall': 1})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "    # Regex to capture the main components of a line\n",
    "    main_pattern = re.compile(r\"^IPRA: Function: (.+?)\\[(.*?)\\]\\s*(.*)$\")\n",
    "    # Regex to find all flag names within the flags part of the line\n",
    "    flag_pattern = re.compile(r\"(\\w+): \\d+\")\n",
    "    parsed_functions = {}\n",
    "    for filepath in files_to_process:\n",
    "        with open(filepath, 'r', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                main_match = main_pattern.match(line.strip())\n",
    "                \n",
    "                if main_match:\n",
    "                    func_name = main_match.group(1)\n",
    "                    cu_name = main_match.group(2)\n",
    "                    flags_string = main_match.group(3)\n",
    "                    present_flags = flag_pattern.findall(flags_string)\n",
    "                    \n",
    "                    parsed_functions[func_name] = present_flags\n",
    "    return parsed_functions\n",
    "\n",
    "\n",
    "parsed_functions = filter_dangerous_functions(LIVENESS_PRERA_DATA_DIR)\n",
    "discarded_functions = set()\n",
    "discarded_counts = defaultdict(int)\n",
    "discarded_flags = {'HasAddressTaken', 'MustTailCall', 'IsInterposable', 'UsesAreIndirectCall', 'AllUsesAreNotCall'}\n",
    "for func in output_data:\n",
    "    if func in discarded_functions or func not in parsed_functions:\n",
    "        continue\n",
    "    flags = set(parsed_functions[func])\n",
    "    intersection = flags.intersection(discarded_flags)\n",
    "    if not intersection:\n",
    "        continue\n",
    "    for flag in intersection:\n",
    "        discarded_counts[flag] += 1\n",
    "    discarded_functions.add(func)\n",
    "print(f\"found {len(discarded_functions)} to discard\")\n",
    "print(discarded_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "019f1931-0409-41d5-92d5-bda72609e3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 63 functions. Now total 1667 functions left.\n"
     ]
    }
   ],
   "source": [
    "before_len = len(output_data)\n",
    "for func in discarded_functions:\n",
    "    output_data.pop(func)\n",
    "after_len = len(output_data)\n",
    "print(f\"Filtered out {before_len - after_len} functions. Now total {len(output_data)} functions left.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a6955051-e0b1-4c21-9be9-6709a1f81548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final filtering, 1667 functions.\n",
      "\n",
      "✅ Successfully merged profile data into './fdo_liveness_output/liveness_profdata.json'\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_FILE = f'{LIVENESS_DATA_DIR}/liveness_profdata.json'\n",
    "filtered_output_data = {func: score for func, score in output_data.items()}\n",
    "print(f\"Final filtering, {len(filtered_output_data)} functions.\")\n",
    "output_dict = {\"functions\": filtered_output_data}\n",
    "with open(OUTPUT_FILE, 'w') as f:\n",
    "    json.dump(output_dict, f, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Successfully merged profile data into '{OUTPUT_FILE}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f884aa6",
   "metadata": {},
   "source": [
    "## Algorithm 2: Propagating Costs via Bottom-Up Call Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3068ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_files(directory):\n",
    "    \"\"\"\n",
    "    Parses all 'ipra_analysis_*.txt' files in a directory to build a model\n",
    "    of the program's call graph and register usage.\n",
    "    \"\"\"\n",
    "    callee_save_costs = {}\n",
    "    function_hotness = {} # Store hotness for each function\n",
    "    callee_call_sites = defaultdict(list)\n",
    "    # The call graph is represented as Caller -> set(Callees)\n",
    "    successors = defaultdict(set)\n",
    "    predecessors = defaultdict(set)\n",
    "\n",
    "    func_pattern = re.compile(r\"IPRA: Function: (.*?)\\[\")\n",
    "    usage_pattern = re.compile(r\"CSRegUsage: (.*?) IsFunctionEntryHot: (\\d+)\")\n",
    "    call_pattern = re.compile(r\"Calls: (.*?)\\[.*\\] IsTailCall: (\\d+).*? LivingCSRegs: (.*)\")\n",
    "    mbb_pattern = re.compile(r\"MBB: \\d+.*?MBBCount: (\\d+)\")\n",
    "\n",
    "    files_to_process = [os.path.join(directory, f) for f in os.listdir(directory) if f.startswith('ipra_analysis_') and f.endswith('.txt')]\n",
    "    print(f\"Found {len(files_to_process)} profile files to process.\")\n",
    "\n",
    "    all_functions = set()\n",
    "\n",
    "    for filepath in files_to_process:\n",
    "        with open(filepath, 'r', errors='ignore') as f:\n",
    "            current_function = None\n",
    "            current_mbb_count = 0\n",
    "            for line in f:\n",
    "                func_match = func_pattern.search(line)\n",
    "                if func_match:\n",
    "                    new_function = func_match.group(1).strip()\n",
    "                    if new_function != current_function:\n",
    "                        current_mbb_count = 0\n",
    "                    current_function = new_function\n",
    "                    all_functions.add(current_function)\n",
    "                    \n",
    "\n",
    "                usage_match = usage_pattern.search(line)\n",
    "                if usage_match and current_function:\n",
    "                    regs_str = usage_match.group(1).strip()\n",
    "                    num_regs = len(regs_str.split()) if regs_str else 0\n",
    "                    callee_save_costs[current_function] = num_regs\n",
    "                    is_hot_str = usage_match.group(2).strip()\n",
    "                    function_hotness[current_function] = (int(is_hot_str) == 1)\n",
    "                \n",
    "                mbb_match = mbb_pattern.search(line)\n",
    "                if mbb_match:\n",
    "                    current_mbb_count = int(mbb_match.group(1))\n",
    "\n",
    "                call_match = call_pattern.search(line)\n",
    "                if call_match and current_function:\n",
    "                    callee_name = call_match.group(1).strip()\n",
    "                    is_tail_call_str = call_match.group(2).strip()\n",
    "                    \n",
    "                    all_functions.add(callee_name)\n",
    "                    live_regs_str = call_match.group(3).strip()\n",
    "                    num_live_regs = len(live_regs_str.split()) if live_regs_str else 0\n",
    "                    \n",
    "                    callee_call_sites[callee_name].append({\n",
    "                        \"caller\": current_function,\n",
    "                        \"live_csrs\": num_live_regs,\n",
    "                        \"count\": current_mbb_count,\n",
    "                        \"is_tail_call\": (int(is_tail_call_str) == 1) # Store tail call info\n",
    "                    })\n",
    "                    successors[current_function].add(callee_name)\n",
    "                    predecessors[callee_name].add(current_function)\n",
    "\n",
    "    print(f\"Found {len(all_functions)} unique functions in the call graph.\")\n",
    "    return callee_save_costs, callee_call_sites, successors, predecessors, all_functions, function_hotness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21a550c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "\n",
    "def tarjan_scc(nodes, successors):\n",
    "    \"\"\"\n",
    "    Tarjan's algorithm.\n",
    "    Args:\n",
    "        nodes: iterable of node ids (e.g., function names)\n",
    "        successors: dict[node] -> set[node] (caller -> callees)\n",
    "    Returns:\n",
    "        sccs: list[list[node]] where each inner list is one SCC (arbitrary order)\n",
    "        comp_id: dict[node] -> int  component index for each node\n",
    "    \"\"\"\n",
    "    index = 0\n",
    "    idx = {}\n",
    "    low = {}\n",
    "    onstack = set()\n",
    "    stack = []\n",
    "    sccs = []\n",
    "\n",
    "    def strongconnect(v):\n",
    "        nonlocal index\n",
    "        idx[v] = index\n",
    "        low[v] = index\n",
    "        index += 1\n",
    "        stack.append(v)\n",
    "        onstack.add(v)\n",
    "\n",
    "        for w in successors.get(v, ()):\n",
    "            if w not in idx:\n",
    "                strongconnect(w)\n",
    "                low[v] = min(low[v], low[w])\n",
    "            elif w in onstack:\n",
    "                low[v] = min(low[v], idx[w])\n",
    "\n",
    "        # root of an SCC\n",
    "        if low[v] == idx[v]:\n",
    "            component = []\n",
    "            while True:\n",
    "                w = stack.pop()\n",
    "                onstack.remove(w)\n",
    "                component.append(w)\n",
    "                if w == v:\n",
    "                    break\n",
    "            sccs.append(component)\n",
    "\n",
    "    for v in nodes:\n",
    "        if v not in idx:\n",
    "            strongconnect(v)\n",
    "\n",
    "    # component id map\n",
    "    comp_id = {}\n",
    "    for cid, comp in enumerate(sccs):\n",
    "        for v in comp:\n",
    "            comp_id[v] = cid\n",
    "    return sccs, comp_id\n",
    "\n",
    "\n",
    "def build_condensed_dag(nodes, successors, predecessors):\n",
    "    \"\"\"\n",
    "    Collapse nodes into SCC super-nodes and build the condensed DAG.\n",
    "    Returns:\n",
    "        sccs: list[list[node]]  original nodes per component id\n",
    "        comp_id: dict[node]->int\n",
    "        dag_succ: dict[cid] -> set[cid]\n",
    "        dag_pred: dict[cid] -> set[cid]\n",
    "    \"\"\"\n",
    "    sccs, comp_id = tarjan_scc(nodes, successors)\n",
    "\n",
    "    dag_succ = defaultdict(set)\n",
    "    dag_pred = defaultdict(set)\n",
    "\n",
    "    for u in nodes:\n",
    "        cu = comp_id[u]\n",
    "        for v in successors.get(u, ()):\n",
    "            cv = comp_id[v]\n",
    "            if cu != cv:\n",
    "                dag_succ[cu].add(cv)\n",
    "                dag_pred[cv].add(cu)\n",
    "\n",
    "    # ensure every component appears in maps\n",
    "    C = len(sccs)\n",
    "    for c in range(C):\n",
    "        _ = dag_succ[c]\n",
    "        _ = dag_pred[c]\n",
    "\n",
    "    return sccs, comp_id, dag_succ, dag_pred\n",
    "\n",
    "\n",
    "def scc_order_bottom_up(dag_succ, dag_pred):\n",
    "    \"\"\"\n",
    "    Bottom-up order on the SCC DAG: sinks first (no outgoing edges).\n",
    "    Uses a Kahn-style peel from sinks upward.\n",
    "    Returns:\n",
    "        order: list[int] of component ids in bottom-up order.\n",
    "    \"\"\"\n",
    "    all_cids = set(dag_succ) | set(dag_pred)\n",
    "    outdeg = {c: len(dag_succ.get(c, ())) for c in all_cids}\n",
    "    q = deque([c for c in all_cids if outdeg[c] == 0])\n",
    "    order = []\n",
    "\n",
    "    while q:\n",
    "        u = q.popleft()\n",
    "        order.append(u)\n",
    "        for p in dag_pred.get(u, ()):\n",
    "            outdeg[p] -= 1\n",
    "            if outdeg[p] == 0:\n",
    "                q.append(p)\n",
    "\n",
    "    # If cycles existed here, something's wrong—condensation must be a DAG.\n",
    "    # But to be safe, append any unprocessed nodes (should be none).\n",
    "    if len(order) != len(all_cids):\n",
    "        remaining = [c for c in all_cids if c not in set(order)]\n",
    "        order.extend(remaining)\n",
    "    return order\n",
    "\n",
    "\n",
    "def function_order_bottom_up(nodes, successors, predecessors):\n",
    "    \"\"\"\n",
    "    Convenience wrapper: returns\n",
    "      - sccs (list of lists of original nodes),\n",
    "      - bottom-up SCC id order,\n",
    "      - a flattened bottom-up function order (each SCC kept as a group).\n",
    "    \"\"\"\n",
    "    sccs, comp_id, dag_succ, dag_pred = build_condensed_dag(nodes, successors, predecessors)\n",
    "    scc_bottom_up = scc_order_bottom_up(dag_succ, dag_pred)\n",
    "\n",
    "    # Flatten functions in bottom-up SCC order.\n",
    "    # For multi-node SCCs (recursion), return them as a list to process as a unit.\n",
    "    grouped = [list(sccs[cid]) for cid in scc_bottom_up]\n",
    "    flat = [f for group in grouped for f in group]  # if you really need a flat list\n",
    "\n",
    "    return {\n",
    "        \"sccs\": sccs,\n",
    "        \"scc_bottom_up_order\": scc_bottom_up,\n",
    "        \"grouped_functions_bottom_up\": grouped,\n",
    "        \"flat_functions_bottom_up\": flat,\n",
    "        \"comp_id\": comp_id,\n",
    "        \"condensed_successors\": dag_succ,\n",
    "        \"condensed_predecessors\": dag_pred,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26565542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_benefits_bottom_up(callee_save_costs, callee_call_sites, successors, predecessors, all_functions, function_hotness, size_penalty, threshold):\n",
    "    \"\"\"\n",
    "    Calculates benefit scores using a bottom-up traversal of the call graph\n",
    "    to model the cascading effects of the preserve_none optimization.\n",
    "    \"\"\"\n",
    "    res = function_order_bottom_up(all_functions, successors, predecessors)\n",
    "    sorted_nodes = res[\"flat_functions_bottom_up\"]\n",
    "    print(f\"Topologically sorted {len(sorted_nodes)} functions for bottom-up processing.\")\n",
    "\n",
    "    final_candidates = set()\n",
    "    # This dictionary simulates how a caller's own save cost might increase\n",
    "    # as its callees become preserve_none.\n",
    "    effective_cs_usage = defaultdict(int, callee_save_costs)\n",
    "    final_scores = {}\n",
    "\n",
    "    for callee in sorted_nodes:\n",
    "        # Skip any non-hot function:\n",
    "        if not function_hotness.get(callee, False):\n",
    "            final_scores[callee] = float('-inf')\n",
    "            continue\n",
    "\n",
    "        # 1. Calculate the benefit for the current function using the most up-to-date\n",
    "        #    cost information for itself and its callees.\n",
    "        callee_cost = effective_cs_usage[callee]\n",
    "        total_dynamic_benefit = 0\n",
    "        sum_of_caller_static_costs = 0\n",
    "        \n",
    "        call_sites = callee_call_sites.get(callee, [])\n",
    "        for site in call_sites:\n",
    "            if site.get(\"is_tail_call\", False):\n",
    "                continue\n",
    "            caller_name = site[\"caller\"]\n",
    "            caller_cost = 6 - site[\"live_csrs\"] #X86-64 has 6 callee-saved registers\n",
    "            \n",
    "            exec_count = site[\"count\"]\n",
    "            total_dynamic_benefit += (callee_cost - caller_cost) * exec_count\n",
    "            sum_of_caller_static_costs += caller_cost\n",
    "\n",
    "        total_static_cost = (2 * sum_of_caller_static_costs) - (2 * callee_cost)\n",
    "        adjusted_score = total_dynamic_benefit - (size_penalty * total_static_cost)\n",
    "        final_scores[callee] = adjusted_score\n",
    "\n",
    "        # 2. Make a decision for the current function.\n",
    "        if adjusted_score > 0:\n",
    "            final_candidates.add(callee)\n",
    "            \n",
    "            # 3. Propagate the cost of this decision upwards to its callers.\n",
    "            #    We assume the cost pushed up is the original, static number of\n",
    "            #    registers the callee was responsible for.\n",
    "            original_callee_cost = callee_save_costs.get(callee, 0)\n",
    "            for caller in predecessors[callee]:\n",
    "                # This simulates the increased register pressure on the caller.\n",
    "                effective_cs_usage[caller] += original_callee_cost\n",
    "\n",
    "    return {func: score for func, score in final_scores.items() if func in final_candidates and score > threshold}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6416cb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1686 profile files to process.\n",
      "Found 82923 unique functions in the call graph.\n",
      "Topologically sorted 82923 functions for bottom-up processing.\n",
      "Found 2280 candidate functions meeting the threshold.\n"
     ]
    }
   ],
   "source": [
    "LIVENESS_DATA_DIR = './thinly_linked_fdo_liveness_output'\n",
    "SIZE_PENALTY = 0.1\n",
    "PRESERVE_NONE_THRESHOLD = 0\n",
    "\n",
    "costs, sites, successors, predecessors, all_nodes, function_hotness = parse_files(LIVENESS_DATA_DIR)\n",
    "candidate_scores = calculate_benefits_bottom_up(costs, sites, successors, predecessors, all_nodes, function_hotness, SIZE_PENALTY, PRESERVE_NONE_THRESHOLD)\n",
    "\n",
    "output_data = dict(candidate_scores.items())\n",
    "\n",
    "print(f\"Found {len(candidate_scores)} candidate functions meeting the threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63b845e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 305 to discard\n",
      "defaultdict(<class 'int'>, {'AllUsesAreNotCall': 300, 'HasAddressTaken': 305, 'UsesAreIndirectCall': 1})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "LIVENESS_PRERA_DATA_DIR = './thinly_linked_fdo_liveness_output'\n",
    "def filter_dangerous_functions(directory):\n",
    "    files_to_process = [os.path.join(directory, f) for f in os.listdir(directory) if f.startswith('ipra_prera_analysis_') and f.endswith('.txt')]\n",
    "    # Regex to capture the main components of a line\n",
    "    main_pattern = re.compile(r\"^IPRA: Function: (.+?)\\[(.*?)\\]\\s*(.*)$\")\n",
    "    # Regex to find all flag names within the flags part of the line\n",
    "    flag_pattern = re.compile(r\"(\\w+): \\d+\")\n",
    "    parsed_functions = {}\n",
    "    for filepath in files_to_process:\n",
    "        with open(filepath, 'r', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                main_match = main_pattern.match(line.strip())\n",
    "                \n",
    "                if main_match:\n",
    "                    func_name = main_match.group(1)\n",
    "                    cu_name = main_match.group(2)\n",
    "                    flags_string = main_match.group(3)\n",
    "                    present_flags = flag_pattern.findall(flags_string)\n",
    "                    \n",
    "                    parsed_functions[func_name] = present_flags\n",
    "    return parsed_functions\n",
    "\n",
    "\n",
    "parsed_functions = filter_dangerous_functions(LIVENESS_PRERA_DATA_DIR)\n",
    "discarded_functions = set()\n",
    "discarded_counts = defaultdict(int)\n",
    "discarded_flags = {'HasAddressTaken', 'MustTailCall', 'IsInterposable', 'UsesAreIndirectCall', 'AllUsesAreNotCall'}\n",
    "for func in output_data:\n",
    "    if func in discarded_functions or func not in parsed_functions:\n",
    "        continue\n",
    "    flags = set(parsed_functions[func])\n",
    "    intersection = flags.intersection(discarded_flags)\n",
    "    if not intersection:\n",
    "        continue\n",
    "    for flag in intersection:\n",
    "        discarded_counts[flag] += 1\n",
    "    discarded_functions.add(func)\n",
    "print(f\"found {len(discarded_functions)} to discard\")\n",
    "print(discarded_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e98f8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 305 functions. Now total 1975 functions left.\n"
     ]
    }
   ],
   "source": [
    "before_len = len(output_data)\n",
    "for func in discarded_functions:\n",
    "    output_data.pop(func)\n",
    "after_len = len(output_data)\n",
    "print(f\"Filtered out {before_len - after_len} functions. Now total {len(output_data)} functions left.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ff5e36",
   "metadata": {},
   "source": [
    "## Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b79cd502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Successfully merged profile data into './thinly_linked_fdo_liveness_output/liveness_profdata.json'\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_FILE = f'{LIVENESS_DATA_DIR}/liveness_profdata.json'\n",
    "output_dict = {\"functions\": output_data}\n",
    "with open(OUTPUT_FILE, 'w') as f:\n",
    "    json.dump(output_dict, f, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Successfully merged profile data into '{OUTPUT_FILE}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13a0981f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created './thinly_linked_fdo_liveness_output/pn.syms' with 1975 function symbols.\n"
     ]
    }
   ],
   "source": [
    "# For pn.syms\n",
    "function_names = list(output_data.keys())\n",
    "PN_SYMS_OUTPUT_PATH = f'{LIVENESS_DATA_DIR}/pn.syms'\n",
    "with open(PN_SYMS_OUTPUT_PATH, 'w') as f:\n",
    "    for name in function_names:\n",
    "        f.write(name + '\\n')\n",
    "print(f\"Successfully created '{PN_SYMS_OUTPUT_PATH}' with {len(function_names)} function symbols.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
